% \usepackage{amssymb}

前節で紹介したRNNでは、隠れ状態$h_t$はその前の隠れ状態$h_{t-1}$と入力状態$x_t$に依存して決定される。言い換えると、データのシークエンスで表される時系列データを、データの先頭から時間に沿って再起的に計算を行っていく。そのため、学習段階でのバッチを用いた並列処理(GPUの有効活用)が困難となる。計算時間の高速化が困難という点以外にも一つ手前の隠れ状態のみを使うために、長期の時系列データを効率よく学習できないという問題点もある。詳しくは前節を参照してほしい。

transformerは複数の機構を組み合わせることで、これらの問題に対する解決を図った。以下ではtransformerにおける重要な構成要素を一つづつ紹介していく。

% 機構は要素間の距離に関係なく依存関係をモデリング出来るという利点を持ち、様々な時系列データを用いたタスクにおいて必要不可欠なものになりつつある。


\subsubsection{エンコーダとデコーダ}
原著論文「Attention is all you need」で提唱されたtransformerはエンコーダ・デコーダ transformerと呼ばれており、その名前の通りエンコーダ層とデコーダ層の二つのサブネットワークから構成されている。このエンコーダ・デコーダ モデルは文章から文章への変換 (Sequence to Sequence、Seq2Seq)が可能で、翻訳問題を扱える。エンコーダ層では入力情報である翻訳元の言語をエンコードし、デコーダ層ではそのエンコードされた情報を元に翻訳先の言語を生成する。

transformerには他にもエンコーダ層のみを持つエンコーダtransformerやデコーダ層のみを持つデコーダtransformerも存在し、それぞれ文章生成や文章のクラスタリング問題などに用いられる。

本節では主に原著で紹介されているエンコーダ・デコーダtransformerに焦点を当てて解説を進めていく。
\subsubsection{Token Embedding}
上記で説明したように今回は翻訳問題を扱う。ここでの翻訳とは文章から文章への変換、例えば日本語の文章から英語の文章への変換を指す。

プログラミングを用いた自然言語処理では文章や言葉をそのまま入力として扱うのではなく、文章をトークンに分けて考える（トークン化と呼ぶ）。トークンとは単語や単語をさらに分割した単位のことを指し、英語の場合は単にスペースで文章を区切る場合が多い。例えば以下の様なトークン化が採用される。


% \begin{quote}
\begin{itemize}
\item 私の趣味はプログラミングです。$\rightarrow$ [私/の/趣味/は/プログラミング/です/]
\item My hobby is programming. $\rightarrow$ [My/Hobby/is/programming/.]
\end{itemize}

全てのデータセット(翻訳対コーパス)のトークン化が完了すると、次にコーパスの中に出現する全ての語彙(トークン)に一意な番号を割り振る。この時に割り振った番号の総数を語彙数と呼び、データセットが大きくなればなるほど語彙数が膨大になる。なおこれらの作業は言語ごとに行われる。

それぞれのトークンに割り振られた番号はその後高次元空間上でのベクトルへと移される。このベクトルは学習を進めることで単語の意味を含むようになり、似た単語はお互い似たベクトルとして表されるようになりると期待される。


\begin{align*}
    \mathbf{W}_e &\in \mathbb{R}^{d_e\times N_V} \\ 
    \mathbf{r} &= \mathbf{W}_e \mathbf{e}_t
\end{align*}

通常は


\subsubsection{Positional Encoding}



\subsubsection{Attention}
\subsubsection{Mask}
\subsubsection{学習と推論}
\subsubsection{transformerを用いたモデル}